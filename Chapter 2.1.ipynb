{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d166852",
   "metadata": {},
   "source": [
    "# Chapter02 텍스트 마이닝 첫걸음\n",
    "\n",
    "- 이장의 핵심개념\n",
    "    - 웹 크롤링으로 데이터를 수집한다\n",
    "    - 키워드 추출의 방법을 알아본다.\n",
    "    - 키워드 간의 연관 관계를 분석한다.\n",
    "    - 텍스트 분석 결과를 시각화 한다.\n",
    "    \n",
    "### 2.1 웹 크롤링으로 기초 데이터 수집하기\n",
    "\n",
    "이번 절에서는 '나무위키 최근 변경 페이지'의 텍스트 데이터르 웹 크롤링 Web Crawling 으로 수집한 다음, 데이터 내에서 등장한 키워드의 출현 빈도를 분석해 보겠다. 이를 통해 우리는 나무위키 페이지에서 현재 가장 '핫한'키워드가 무엇인지 분석할 수 있다. 웹 크롤링 혹은 웹 스크래핑 Web Scraping은 인터넷에 있는 웹페이지를 방문해서 페이지의 자료를 자동으로 수집하는 작업을 의미한다. 여기에선 파이썬으로 웹 크롤링을 진행하겠다.\n",
    "\n",
    "#### 대상 페이지의 구조 살펴보기\n",
    "\n",
    "크롤링을 위한 첫 번째 단계는 인터넷 익스플로러, 크롬 등의 웹브라우저를 실행하여 크롤링의 대상이 될 페이지 구조를 살펴보는 것이다. \n",
    "- 먼저 웹 브라우저의 '개발자 도구'를 실행한다. 단축키 Ctrl + Shift + l\n",
    "- 리스트의 URL 정보를 수집\n",
    "\n",
    "#### 웹 크롤링 라이브러리 사용하기\n",
    "\n",
    "파이썬에서는 BeautifulSoup 과 requests라는 라이브러리로 웹 크롤러를 만들 수 있다. requests는 특정 URL로부터 HTML 문서를 가져오는 작업을 수행하고, BeautifulSoup 모듈은 HTML 문서에서 데이터를 추출하는 작업을 수행한다. 이 모듈들을 사용하기에 앞서, 터미널(cmd) 혹은 아나콘다 프롬프트를 실행하여 아래의 세 가지 파이썬 모듈을 설치해야한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf085768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Using cached https://files.pythonhosted.org/packages/1f/1d/a4485412268b38043a6c0f873245b5d9315c6615bcf44776759a2605dca5/lxml-4.6.3-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting beautifulsoup4\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl\n",
      "Collecting requests\n",
      "  Using cached https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl\n",
      "Collecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4)\n",
      "  Using cached https://files.pythonhosted.org/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl\n",
      "Collecting chardet<5,>=3.0.2 (from requests)\n",
      "  Using cached https://files.pythonhosted.org/packages/19/c7/fa589626997dd07bd87d9269342ccb74b1720384a4d739a1872bd84fbe68/chardet-4.0.0-py2.py3-none-any.whl\n",
      "Collecting urllib3<1.27,>=1.21.1 (from requests)\n",
      "  Using cached https://files.pythonhosted.org/packages/09/c6/d3e3abe5b4f4f16cf0dfc9240ab7ce10c2baa0e268989a4e3ec19e90c84e/urllib3-1.26.4-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/a0/5f06e1e1d463903cf0c0eebeb751791119ed7a4b3737fdc9a77f1cdfb51f/certifi-2020.12.5-py2.py3-none-any.whl\n",
      "Collecting idna<3,>=2.5 (from requests)\n",
      "  Using cached https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl\n",
      "Installing collected packages: lxml, soupsieve, beautifulsoup4, chardet, urllib3, certifi, idna, requests\n",
      "Successfully installed beautifulsoup4-4.9.3 certifi-2020.12.5 chardet-4.0.0 idna-2.10 lxml-4.6.3 requests-2.25.1 soupsieve-2.2.1 urllib3-1.26.4\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install lxml beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bcb339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743a273e",
   "metadata": {},
   "source": [
    "다음 코드에서는 requests.get()함수로 URL의 HTML 문서를 가져온 뒤, 이를 BeautifulSOup()클래스의 soup 객체로 변환한다. 그리고 find(), find_all() 함수를 사용하여 특정 HTML 태그 혹은 특정 HTML 클래스를 가진 데이터를 가져온다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8f3e5",
   "metadata": {},
   "source": [
    "- 페이지 리스트 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0042f140",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-05e551bfbc9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcontents_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"table\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"table-hover\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtable_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontents_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tbody\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtable_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable_body\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# 크롤링할 사이트 주소를 정의한다\n",
    "source_url = \"https://namu.wiki/RecentChanges\"\n",
    "\n",
    "# 사이트의 HTML 구조에 기반하여 크롤링을 수행한다.\n",
    "req = requests.get(source_url)\n",
    "html = req.content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "contents_table = soup.find(name = \"table\", attrs = {\"class\":\"table-hover\"})\n",
    "table_body = contents_table.find(name=\"tbody\")\n",
    "table_rows = table_body.find_all(name=\"tr\")\n",
    "\n",
    "# a 태그의 href 속성을 리스트로 추출하여 크롤링할 페이지 리스트를 생성한다.\n",
    "page_url_base = \"https://namu.wiki\"\n",
    "page_urls = []\n",
    "for index in range(0, len(table_rows)):\n",
    "    first_td = table_rows[index].find_all('td')[0]\n",
    "    td_url = first_td.find.all('a')\n",
    "    if len(td_url) > 0:\n",
    "        page_url = page_url_base + td_url[0].get('href')\n",
    "        page_urls.append(page_url)\n",
    "        \n",
    "# 중복 url을 제거한다.\n",
    "page_urls = list(set(page_urls))\n",
    "for page in page_urls[:5]:\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de1d1648",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-37d5e985829f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcontents_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtable_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontents_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tbody\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtable_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable_body\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# 크롤링할 사이트 주소를 정의합니다.\n",
    "source_url = \"https://namu.wiki/RecentChanges\"\n",
    "\n",
    "# 사이트의 html 구조에 기반하여 크롤링을 수행합니다.\n",
    "req = requests.get(source_url)\n",
    "html = req.content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "contents_table = soup.find(name=\"table\")\n",
    "table_body = contents_table.find(name=\"tbody\")\n",
    "table_rows = table_body.find_all(name=\"tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748191e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
